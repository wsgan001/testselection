\section{Introduction} \label{sec:introduction}
 
Access control is one of the privacy and security mechanisms for granting only legitimate users with access to critical information. 
Access control is governed by access control policies (policies in short), each of which includes a sequence of rules to specify 
which subjects are permitted or denied to access which resources under which conditions. To facilitate specifying and maintaining policies separately from entangled code, 
system developers specify and enforce policies separated from actual functionality (i.e., business logic) of a system.

In the system, program code, which represents the actual functionality,
interacts with a policy through a security component, called Policy Decision Point (PDP).
Consider that the program code consists of methods $Ms$ including Policy Enforcement Points (PEPs).
PEPs are functionalities that require decisions (e.g., permit or deny) whether a given subject can have access on protected information.

Typically, PEPs in $Ms$ formulate an access request that specifies that a subject would like to have a specific type of access (e.g., read or write) on specific protected information. 
The PEPs next submit the request to a PDP, which evaluates the request against the policy loaded on the PDP and determines whether the request should be permitted or denied. Finally, 
the PDP formulates and sends the
decision back to the PEPs to proceed.

As security requirements of software often change during software development and maintenance,
developers may modify policies to comply with requirements. In order to increase confidence that the modification of policies is correct and
does not introduce unexpected behavior, developers periodically conduct regression testing.
For example, new security requirements include new security concerns to be added into a policy.
Developers may change policies without changing program code related to actual system functionality.
In such a situation, validating and verifying program code and policies together after policy changes
increases confidence of correct behaviors of the given system.


In this paper, we focus on the regression testing problem in the context of policy evolution.
For policy evolution, regression testing is important because policy behavior changes may
result in unexpected behaviors of program code, these behaviors can be undesirable. Suppose, for example, that there is some errors 
in the access control mechanisms so that unauthorized access is gained to control system' resources, such errors can be used to bypass the security policy and may have a deep impact on the application's security.
Prior test selection research work ~\cite{Rothermel:1996:ART:235681.235682,Graves:2001:ESR:367008.367020,Elbaum:2000:PTC:347324.348910}
deals with changes in program code. However, this work does not consider code-related components such as policies that interact with the program code.
To the best of our knowledge,
our paper is the first one for automatic test-selection approach in context of policy
evolution.

The typical regression testing for program code interacting with a policy is as follows.
Given a program and a policy $P$, the developers prepare initial system test cases, where
each test case maps to rules (in the policy) exercised by the test case. Given $P$ and its modified
policy $P'$, the developers compare impacts of $P$ and $P'$ to
reveal different policy behaviors, which are ``dangerous'' portions to be validated with
test cases. For validating these ``dangerous'' portions, the developers often select the test cases (from test cases for $P$) that exercise the dangerous
portions of $P'$.

For regression testing, instead of writing new system test cases, developers reuse the initial test cases in practice. The naive regression testing strategy is to rerun all system test cases. However,
 this strategy is costly and time-consuming, especially for large-scale systems. Moreover, if the number of the initial 
system test cases is large, this strategy may require significant time for developers to conduct testing. In order to
 reduce the cost of regression testing, developers often adopt regression test selection, which selects and executes only
 test cases to expose different behaviors across different versions of the system. This approach also requires substantial
 costs to select and execute such system test cases from the initial test cases that could reveal faults introduced by the changes. 
If the cost of regression test selection is smaller than rerunning all of initial system test cases, test selection helps reducing overall cost in validating whether the modification is correct. 


In order to address the issue, we propose a regression-test-selection approach, which selects every test case that may 
reveal different behaviors in program code impacted by policy changes.
In general, our approach automatically compares an original 
policy $P$ and its modified policy $P'$ to detect different policy behaviors. In the policy context, different policy 
behaviors refer to that, given a request, its evaluated decisions (for $P$ and $P'$, respectively) are different.
These different policy behaviors are dangerous portions to be validated.
As a policy consists of rules, these different policy behaviors are induced by rules impacted by policy changes.

%modified policy behaviors,
%which are induced by rules impacted by policy change.
%As a policy consists of rules, we refer to dangerous portions as rules impacted by policy changes. 
%We next find test cases impacted by policy changes.


%------------------------- 
%In this paper, we propose an automated test-selection approach
%and test-augmentation technique,
%three automated test-selection techniques and one : 
%-------------------------

Our test-selection approach compares three techniques related to access control testing:
The first one is based on mutation analysis (that converts each rule's decision in turn and executes 
and finds test cases related to each rule). The second one is based on coverage analysis (that records 
which rules are evaluated by executing each test case), and the third one is based on evaluated decisions of requests issued from test cases. 
Our approach next selects only test cases that execute program code impacted by policy changes.

While test-selection techniques are useful for selecting test cases for program code impacted by policy changes, these test cases may not 
cover all the rules that are impacted by those policy changes.
To address this issue, we propose a test-augmentation technique, which complements our test-selection techniques.
Note that we first use a regression-test-selection technique before using the test-augmentation technique that generates the additional test cases 
that cover changed policy behavior not covered by existing test cases.

\Comment{
While test-selection techniques are useful for selecting test cases for program code impacted by policy changes, these test cases may not sufficiently cover all the rules in the policy impacted by the changes.
To address this issue, we propose a test-augmentation technique, which complements our test-selection techniques.
Note that we first use a regression-test-selection technique before using the test-augmentation technique to achieve
additional policy coverage for not-covered but impacted rules $NR$ by existing test cases.
Our test-augmentation technique analyzes and recommends existing test cases, which can issue requests $NR_{req}$ cover $NR$
after manual modification of the test cases.
In the technique, we use prioritization to classify test cases into various sets based on their modification likelihood
to issue $NR_{req}$.
High likelihood may lead to less number of test case modification to issue $NR_{req}$.
Given test cases with high likelihood, we manually modify and augment the test cases to increase coverage of $NR$.
}

%For augmented test cases,
%we then verify that augmented test cases increase coverage of $N_r$.
%For example, an attribute change (e.g., subject role change from $Student$ to $Professor$) may cover $N_r$.



%structural test generation before using combinatorial test generation to augment the generated test inputs for achieving the
%n-wise (e.g., pairwise) coverage criterion. The reason for
%using combinatorial test generation after structural test generation is that structural test generation (white-box testing)
%cannot detect some types of faults including omission faults
%(e.g., missing a rule).



This paper makes three main contributions:

\begin{itemize}
  \item We develop a test selection approach to select every test case from existing test cases to test program code impacted by policy changes. Our approach
  uses three techniques; the first one is based on mutation analysis, the second one is based on coverage analysis, and the third one is based on evaluated 
decisions of requests issued from test cases. 
  \item We develop a test augmentation technique to generate additional new test cases to cover not-covered but impacted rules with selected test cases by the preceding techniques.

  \item We evaluate our approach on three real world Java programs interacting with policies. Our evaluation results show that our test selection techniques achieve
 up to 51\%$\sim$97\% of test reduction for a modified version with given 5$\sim$25 policy changes. Among our three techniques, our results show that the 
third technique is the most efficient compared with the first
  and the second techniques in terms of elapsed time. The third technique is 43 and 8 times
faster than the first and the second techniques, respectively. Our evaluation results show that our test augmentation technique generates additional test cases to cover 100\% of the impacted rules by policy changes.
\end{itemize}

The rest of the paper is organized as follows.
Section~\ref{sec:background} presents background information about
policy-based software systems, policy context, and regression testing.
Section~\ref{sec:approach} presents our approach.
Section~\ref{sec:implementation} presents our implementation. 
Section~\ref{sec:experiment} describes the evaluation results
where we apply our approach on three projects.
Section~\ref{sec:discussion} discusses issues. 
Section~\ref{sec:related} discusses related
work. Section~\ref{sec:conclusion}
concludes the paper.